
# Wavel Reading Note

Based on commit: https://github.com/Chivier/wavel/commit/7b7b9650fdc10c8ec1bb96ab360592b8d4622041

## A* mapper mapping chain
### Initialization Chain

```
map()
├─ OperatorPredictor.__init__()
├─ MemoryModel.__init__()
│ └─ _build_tensor_sizes()
├─ ExpansionManager.set_graph()
├─ HeuristicCalculator.__init__()
├─ SolutionEvaluator.__init__()
├─ OperatorPartitioner.__init__()
├─ SolutionCollector.__init__()
├─ BestPathLeafCollector.__init__()
├─ CutHelper.__init__()
├─ analyze_cross_group_dependencies()
│ └─ GroupDependencyGraph.from_dependencies()
├─ DependencyAwareAllocator.__init__()
│ └─ GlobalPEAllocator.__init__()
├─ DependencyAwareAllocator.allocate_chain_groups()
├─ TuneHelper.__init__()
└─ SpatialIR.__init__()

```

  

### Mapping Chain

```
map()
└─ _map_groups_parallel() OR _map_group()
├─ _compute_dependency_levels()
└─ _map_group()
├─ _create_root_node()
│ └─ _find_optimal_initial_region()
├─ _lookup_op_group_cache()
├─ SmallGroupOptimizer.optimize()
└─ A* Search Loop
├─ _expand_node()
│ ├─ ExpansionManager.expand()
│ │ ├─ SpatialCutStrategy.apply()
│ │ ├─ TemporalCutStrategy.apply()
│ │ ├─ EdgeTuneStrategy.apply()
│ │ └─ GemmTuneStrategy.apply()
├─ _compute_heuristic()
│ └─ HeuristicCalculator.compute_heuristic()
├─ _evaluate_complete_solution()
│ └─ SolutionEvaluator.evaluate()
└─ _apply_mapping_to_spatial_ir()
└─ SolutionCollector.collect()

```

  

### Post-Processing Chain

```
map()
├─ _calculate_pe_utilization()
├─ _compress_pe_allocation()
├─ _build_timeline()
├─ shrink_idle_pes()
└─ MappingResult.__init__()
```


## Evaluation Flow
```
During A* Search:
  For each node:
    f_cost = HeuristicCalculator.compute_heuristic(node)
             ├─ g_cost: Sum of CostModel.compute_op_cost() for fixed groups
             └─ h_cost: Estimate for remaining groups
    
    MemoryModel.check_memory_feasibility(node) → Prune if infeasible

When complete solution found:
  total_cost = SolutionEvaluator.evaluate(node, graph)
               ├─ For each leaf: CostModel.compute_op_cost()
               ├─ Communication: CostModel.compute_communication_cost()
               └─ Dependency scheduling: Sequential vs parallel execution
```


## High-Level Solution Selection Process

1. A* search algorithm
-   Uses A* to explore the mapping space
-   Each SearchNode represents a partial mapping (some operators assigned to PE regions)
-   Maintains an open set (frontier) and a closed set (visited states)

2. Solution generation
-   Starts from an initial node (typically no operators mapped)
-   Expands nodes by assigning unmapped operators to PE regions
-   Generates candidate successors for each expansion step
-   Prunes invalid or dominated candidates

3. Solution evaluation

-   Two evaluation paths:
-   Subgraph evaluator (use_subgraph_eval=True): Detailed simulation using SubgraphEvaluator with SpatialPlan objects for accurate scheduling
-   Legacy cost model (use_subgraph_eval=False): Faster heuristic using CostModel.compute_op_cost() for individual operations and CostModel.compute_communication_cost() for data movement
-   Evaluation computes total execution cost (computation + communication)

4. Selection criteria

-   A* uses f(n) = g(n) + h(n):
-   g(n): Actual cost from start to current node
-   h(n): Heuristic estimate to goal (remaining operators)
-   Explores nodes with lowest f(n) first
-   Tracks the best complete solution found

5. Termination

-   Stops when:
-   A complete solution is found and no better path exists
-   Search budget (time/iterations) is exhausted
-   Open set is empty (no valid solutions)

6. Result

-   Returns the best complete mapping found (all operators assigned to PE regions with minimal total cost)

The process balances exploration (trying different mappings) with exploitation (focusing on promising paths) to find an efficient spatial mapping.


## Where to Add a New Operator

### 1. Core operator definition (required)

**File: `src/wavel/ir/graph.py`**

**A. Shape inference** (required)
- Add a case in `GraphIR._infer_output_shape()` (around line 439-702)
- Compute output shape from inputs and attributes
- Example pattern:
```python
elif op_type == "your_operator":
    # Extract input shapes
    input_shape = self.tensors[inputs[0]].shape
    # Compute output shape based on operator semantics
    return computed_output_shape
```

**B. Data type inference** (optional, defaults to first input's dtype)
- Add to `GraphIR._infer_output_dtype()` if needed (around line 430)

**C. Memory estimation** (optional)
- Add to `_get_operator_internal_memory()` (around line 108) if the operator needs temporary buffers
- Add to `_get_operator_memory_consumption()` (around line 64) if special handling is needed

### 2. Functional API wrapper (optional, for user convenience)

**File: `src/wavel/functional.py`**

Add a function following this pattern:
```python
def your_operator(input: Tensor, param: float = 1.0, name: Optional[str] = None) -> Tensor:
    g = _get_current_graph()
    if g is None:
        raise RuntimeError("your_operator() must be called within a graph context")
    
    output_name = name or f"your_op_out_{len(g.ops)}"
    g.add_op("your_operator", inputs=[input.name], outputs=[output_name], 
             param=param)  # attributes go here
    
    output_shape = g.tensors[output_name].shape if output_name in g.tensors else None
    return Tensor(output_name, output_shape, input.dtype)
```

### 3. Performance prediction (optional, for optimization)

**A. Cost model integration**
**File: `src/wavel/mapping/evaluator/cost_model.py`**

Add handling in `CostModel.compute_op_cost()` (around line 92-206):
- Add a case for your operator type
- Use predictors, analyzers, or heuristics to estimate execution cycles

**B. Operator size analyzer**
**File: `src/wavel/analysis/operator_size_analyzer.py`**

Add FLOP calculation in `_calculate_flops()` (around line 364-451):
- Compute FLOPs based on input shapes and operator semantics
- Used by the cost model for heuristic estimation

**C. Hardware-specific predictors** (optional, for ML-based prediction)
- Create predictor in `src/predictor/operations/your_operator/`
- Register via `PredictorFactory.register()` in `src/predictor/base.py`
- Add to `src/predictor/api.py` if exposing via unified API

### 4. Special operator categories (if applicable)

**File: `src/wavel/ir/graph.py`**

**A. View-like operations** (zero-cost metadata ops)
- Add to `VIEW_LIKE_OPS` set (around line 50) if your operator doesn't move data
- Examples: `reshape`, `view`, `transpose`, `slice`

**B. In-place operations**
- Add to `INPLACE_UNARY_OPS` or `INPLACE_BINARY_OPS` in `src/wavel/ir/spatial.py` (around line 35) if applicable
- Indicates no additional memory allocation

### 5. Documentation (recommended)

**File: `docs/03-IR/graph_ir.md`**
- Add entry to the operator table with:
  - Operator name
  - Input/output shapes
  - Attributes
  - Example usage

**File: `docs/03-IR/OPERATOR_IMPLEMENTATION_LOCATIONS.md`**
- Document where your operator is implemented

## Information to Provide

### Minimum required
1. Operator name (string identifier)
2. Input/output tensor names
3. Output shape inference logic (from inputs and attributes)
4. Operator attributes (if any)

### Recommended for optimization
5. FLOP count calculation (for cost estimation)
6. Memory requirements (output size + temporary buffers)
7. Performance characteristics:
   - Compute-bound vs memory-bound
   - Parallelization potential
   - Optimal PE allocation shape

### Optional for advanced features
8. Hardware-specific predictors (ML models)
9. Special handling in mapping algorithms
10. Execution semantics (for graph execution)

## Example: Adding a Simple Operator

For a hypothetical `clip` operator (clamp values):

1. Shape inference (`graph.py`):
```python
elif op_type == "clip":
    return self.tensors[inputs[0]].shape  # Same shape as input
```

2. Functional API (`functional.py`):
```python
def clip(x: Tensor, min_val: float = 0.0, max_val: float = 1.0, name: Optional[str] = None) -> Tensor:
    g = _get_current_graph()
    if g is None:
        raise RuntimeError("clip() must be called within a graph context")
    output_name = name or f"clip_out_{len(g.ops)}"
    g.add_op("clip", inputs=[x.name], outputs=[output_name], 
             min=min_val, max=max_val)
    output_shape = g.tensors[output_name].shape if output_name in g.tensors else None
    return Tensor(output_name, output_shape, x.dtype)
```

3. Cost model (`cost_model.py`):
```python
elif op.op_type == "clip":
    # Element-wise operation - similar to relu
    if graph and op.outputs:
        output_tensor = graph.tensors.get(op.outputs[0])
        if output_tensor and hasattr(output_tensor, 'num_elements'):
            elements = output_tensor.num_elements()
            if elements:
                return max(100, elements / pe_count)
```

4. FLOP calculation (`operator_size_analyzer.py`):
```python
elif op_type == "clip":
    if op.outputs:
        output_info = self.get_tensor_info(op.outputs[0])
        if output_info:
            return output_info.num_elements  # 1 comparison per element
```

## Summary Checklist

- [ ] Shape inference in `graph.py` (`_infer_output_shape`)
- [ ] Functional API wrapper in `functional.py` (optional)
- [ ] Cost estimation in `cost_model.py` (`compute_op_cost`)
- [ ] FLOP calculation in `operator_size_analyzer.py` (`_calculate_flops`)
- [ ] Add to `VIEW_LIKE_OPS` if zero-cost (optional)
- [ ] Add to in-place ops sets if applicable (optional)
- [ ] Hardware-specific predictor (optional, advanced)
- [ ] Documentation updates (recommended)

The minimum to make an operator work is shape inference. The rest improves usability and optimization quality.


## How Return Values Are Handled

### 1. Multiple outputs are supported

**In GraphIR (`src/wavel/ir/graph.py`)**:
- Tensors have an `is_output` boolean flag (line 202)
- Multiple tensors can be marked as outputs
- The executor returns a dictionary of all output tensors:

```python
# From executor.py (lines 247-253)
outputs = {}
for tensor_name, tensor in self.graph.tensors.items():
    if tensor.is_output:
        outputs[tensor_name] = self._get_tensor_value(tensor_name)
return outputs  # Returns Dict[str, np.ndarray]
```

**Text format parsing (`src/wavel/compiler/pipeline.py` lines 435-440)**:
- Multiple `return` statements are parsed:
```python
if line.startswith('return '):
    output_match = re.search(r'return\s+%(\w+)', line)
    if output_match:
        output_tensors.add(output_match.group(1))
```

**Serialization (`graph.py` lines 920-928)**:
- Multiple returns are written:
```python
for name in output_tensors:
    lines.append(f"    return %{name}")
```

### 2. Mapping algorithm behavior

The mapping algorithm does not treat outputs specially:
- It maps all operators in the provided groups
- It does not look for "sink" or "output" operators
- It processes operators based on dependencies, not output status
- Output tensors are just tensors with `is_output=True`

From `astar_mapper.py` (line 377):
```python
def map(self, graph: GraphIR, groups: List[List[OpNode]]) -> MappingResult:
    # Maps ALL operators in groups - no special handling for outputs
```

### 3. Answer: return multiple values directly

For two jobs planned as one:

- Option 1: Return two values directly (recommended)
```python
# Job 1 produces output1
output1 = some_operations(...)

# Job 2 produces output2  
output2 = other_operations(...)

# Mark both as outputs
wavel.output(output1)
wavel.output(output2)

# Or in text format:
# return %output1
# return %output2
```

- Option 2: Add a sink operator (not needed)
```python
# This is unnecessary - the mapping algorithm doesn't need it
sink_op = graph.add_op("sink", inputs=[output1, output2], outputs=[])
```

### Why no sink operator is needed

1. The mapping algorithm maps all operators regardless of output status
2. The executor collects all tensors with `is_output=True`
3. Multiple outputs are natively supported
4. A sink operator adds overhead without benefit:
   - Extra operator to map
   - Extra memory allocation
   - No functional purpose (outputs are already tracked)

### How to combine two jobs

**Example:**
```python
with wavel.graph("combined_jobs") as g:
    # Job 1
    x1 = wavel.input("x1", shape=[10, 20])
    w1 = wavel.parameter("w1", shape=[20, 30])
    y1 = wavel.matmul(x1, w1)
    wavel.output(y1)  # First output
    
    # Job 2 (can be independent or share inputs)
    x2 = wavel.input("x2", shape=[15, 25])
    w2 = wavel.parameter("w2", shape=[25, 35])
    y2 = wavel.matmul(x2, w2)
    wavel.output(y2)  # Second output
    
    # Both jobs will be mapped together on the same hardware
```

The grouping and mapping algorithms will:
- Process all operators from both jobs
- Respect dependencies between them (if any)
- Map them to the same hardware PE array
- Return both outputs in the result dictionary

### Summary

- Return multiple values directly using multiple `return` statements or multiple `wavel.output()` calls
- Do not add a sink operator; it's unnecessary and adds overhead
- The IR and mapping system handle multiple outputs natively
- All operators are mapped regardless of whether they produce outputs

The mapping algorithm treats all operators equally—output status only affects what the executor returns, not how operators are mapped to hardware.

Overall search flow
```
GraphIR (operators + dependencies)
    ↓
[Grouping Phase] → Groups of operators
    ↓
[For Each Group]:
    ├─ Create Root Node (all ops, initial PE region)
    ├─ A* Search:
    │   ├─ Expand: CUT (partition) or TUNE (optimize PE region)
    │   ├─ Evaluate: Compute heuristic cost h(n) = g(n) + h(n)
    │   ├─ Prune: Remove high-cost nodes
    │   └─ Select: Best solution found
    └─ Apply: Convert SearchNode tree → SpatialIR mappings
    ↓
SpatialIR (operators mapped to PE coordinates)
```
<!--stackedit_data:
eyJoaXN0b3J5IjpbMjAyODQyNDk5NSwyMDc1ODU1NjYxLDE3Mz
czODI4NSwxNzE3MTcxNjMsODE4NjM2MTI5LC0yMDE1NTY2ODYy
LDE0ODgwNjQxODQsLTIwMDcxNDU3MTMsLTc5NTQzNzI4M119
-->