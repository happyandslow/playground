

# Temporal -awareness policies

Link to commit (https://github.com/happyandslow/wavel/commit/cf0c01c961672ad56e0392e35bbeb3d989fae38d)

## Summary

### 1. Created `temporal_estimator.py` module
- Added `estimate_operator_time_ranges()` as a pure function for timeline computation
- Supports both SubgraphEvaluator (accurate) and simple sequential (fallback) modes
- Separates timeline computation from cost evaluation

### 2. Added `operator_time_ranges` as default field in `SearchNode`
- Added as optional field: `operator_time_ranges: Optional[Dict[str, Dict[str, int]]] = None`
- Provides a clear contract for evaluators to access temporal information

### 3. Refactored `CycleBasedEvaluator`
- Now uses `estimate_operator_time_ranges()` utility instead of duplicating logic
- Timeline computation happens in `evaluate_node()` before cost evaluation
- Methods simplified to return only cost (temporal info attached to node separately)

### 4. Implemented `TemporalFairnessEvaluator`
- Evaluates plans based on temporal PE usage constraints per user
- Detects violations where users exceed their percentage constraints at any time
- Scores violations as: `violation_cost = magnitude × duration × weight`
- Includes methods for:
  - Building PE usage timeline from SearchNode or MappingResult
  - Extracting PE coordinates per operator
  - Calculating violation costs

### 5. Updated `new_astar_mapper.py`
- Added support for `evaluator="temporal_fairness"` option
- Added `user_constraints` parameter to `__init__()`
- Validates that `user_constraints` is provided when using temporal fairness evaluator
- Updated `_build_mapping_result()` to use the temporal estimator utility

### 6. Created `TemporalFairnessThenCycleEvaluator`

A lexicographic evaluator that combines `TemporalFairnessEvaluator` and `CycleBasedEvaluator`:

1. Priority: Temporal fairness violation cost (lower is better)
   - Plans with no violations (cost = 0.0) are prioritized
   - Plans with violations are ranked by violation cost

2. Tie-breaker: Cycle cost (only when temporal fairness costs are equal)
   - Uses a threshold comparison (1e-6) to determine equality
   - When temporal fairness costs are equal, lower cycle cost wins
  
### Usage Example

```python
mapper = NewAStarMapper(
    device=device,
    evaluator="temporal_fairness",
    user_constraints={"user1": 0.2, "user2": 0.3, "user3": 0.5},
    use_subgraph_eval=True
)
```

```python
mapper = NewAStarMapper(
    device=device,
    evaluator="temporal_fairness_then_cycle",
    user_constraints={"user1": 0.2, "user2": 0.3, "user3": 0.5},
    use_subgraph_eval=True
)
```

## Example Scenarios

 3 Users with input sizes 1024, 2048, 4096 respectively

**Subject to 10% PE usage respectively**
![enter image description here](https://raw.githubusercontent.com/happyandslow/playground/refs/heads/main/research/notes/figures/1-4-26/temporal-fairness/spatial_mapping_figures/spatial_mapping_result_map_1024x2048x4096_50000-0.1.iters.png)
Snapshot of usage overtime
![enter image description here](https://raw.githubusercontent.com/happyandslow/playground/refs/heads/main/research/notes/figures/1-4-26/temporal-fairness/timeline_mapping_figures/spatial_mapping_result_map_1024x2048x4096_50000-0.1.iters.png)

**Subject to 5% PE usage respectively**
![enter image description here](https://raw.githubusercontent.com/happyandslow/playground/refs/heads/main/research/notes/figures/1-4-26/temporal-fairness/spatial_mapping_figures/spatial_mapping_result_map_1024x2048x4096_50000-0.05.iters.png)
![enter image description here](https://raw.githubusercontent.com/happyandslow/playground/refs/heads/main/research/notes/figures/1-4-26/temporal-fairness/timeline_mapping_figures/spatial_mapping_result_map_1024x2048x4096_50000-0.05.iters.png)

# **Fairness in Multi-Tenant Resource Sharing: Key Research (2010–2025)**

  

## **Introduction**

  

In multi-tenant computing environments – whether in cloud datacenters or traditional operating systems – **fairness** is a central concern. When multiple users or workloads share CPUs, memory, I/O, or other resources, the system must allocate these resources such that no tenant is unfairly starved or advantaged. Broadly, fairness can be enforced through **algorithmic mechanisms** (scheduling algorithms that split resources in real-time) and **policy-level definitions** (high-level rules or share guarantees configured for users or groups). A classic baseline is **max-min fairness**, which allocates resources such that any increase in one user’s allocation would force a decrease for another user with equal or lesser allocation . In practice, simple equal sharing is often refined by weights or multi-resource considerations to reflect priorities and diverse demands. Over the past 15 years, numerous influential works have defined fairness more rigorously and designed new schedulers to achieve it. Below, we survey several well-cited research contributions, summarizing each paper’s main ideas, target deployment scenario, and definition of fairness.

  

## **OS-Level Fair Scheduling – Linux CFS (2007)**

-   **Main idea:** The Linux **Completely Fair Scheduler (CFS)** exemplifies OS-level fairness by aiming to give each process an equal share of CPU time. CFS is essentially an implementation of _weighted fair queueing (WFQ)_ for CPU scheduling, dividing CPU cycles among runnable threads in proportion to their weights (priority) . This model “_does away with_” fixed time-slice quotas of earlier schedulers and instead continuously balances execution time so that no task runs more than its fair share .
    
-   **Deployment scenario:** CFS became the default scheduler in Linux 2.6.23+ and is used in traditional operating systems on single machines . It manages CPU time across all processes (or groups of processes) on a core, maintaining per-core run queues and selecting the next task to run based on who has used the least CPU so far (tracked via a virtual runtime) . This design keeps the scheduler _work-conserving_ (no CPU sits idle if tasks are waiting) and scales to multi-core systems with per-CPU queues.
    
-   **Fairness definition:** CFS defines fairness as **proportional equal-progress** for tasks. In an ideal sense, if _N_ tasks are running with equal weight, each should get ~1/_N_ of the processor. Threads with higher weight (lower “niceness”) simply get a larger fraction proportionally . By continuously picking the task with the smallest accumulated runtime to run next, CFS ensures that over the long term each task’s CPU time converges to its fair share . This prevents any single process from hogging the CPU and provides _isolation_: even if one task would use 100% CPU, it cannot exceed its allotted share while others are runnable.
    

  

## **Delay Scheduling for Cluster Fairness (Zaharia et al., 2010)**

-   **Main idea:** _Delay Scheduling_ was introduced to reconcile fairness with data locality in cluster computing (specifically Hadoop MapReduce) . In a multi-user Hadoop cluster, a **fair scheduler** tries to give each job an equal share of running tasks (“slots”). However, strictly enforcing fairness can hurt performance if a job’s tasks are forced to run on nodes without the needed data. Zaharia et al. propose a simple tweak: if the job that should be scheduled next (to maintain fairness) cannot launch a task on a local node, it **waits briefly**, allowing another job to use the slot . This short delay lets other tasks run (improving cluster utilization and locality) without significantly violating fairness over time.
    
-   **Deployment scenario:** Delay scheduling was implemented in Facebook’s 600-node Hadoop cluster as part of a fair share scheduler . It targets **datacenter batch processing frameworks** where jobs are divisible into many tasks. The mechanism is used within a single-framework scheduler (Hadoop’s Fair Scheduler) to balance multiple users’ jobs on a shared cluster.
    
-   **Fairness definition:** The underlying fairness policy here is **slot-based max-min fairness** – each user (or job) should get an approximately equal number of task slots over the cluster. Delay scheduling ensures this _over the long run_ while yielding momentarily on strict ordering to improve data locality. Crucially, the algorithm was shown to “increase throughput by up to 2× **while preserving fairness**” – i.e. jobs still receive equal shares of runtime on average . Fairness is measured by each job’s share of executed tasks: delay scheduling doesn’t change each job’s overall share; it only postpones a job for a few seconds if it can’t get a local slot, thereby achieving **nearly optimal locality without sacrificing fairness** .
    

  

## **Dominant Resource Fairness (Ghodsi et al., 2011)**

-   **Main idea:** **Dominant Resource Fairness (DRF)** is a landmark framework that generalizes fair sharing to environments with _multiple resource types_ (CPU, memory, etc.) . Ghodsi et al. observe that traditional fair schedulers reduced multi-resource allocation to a single dimension (e.g. “slots” with fixed CPU and RAM), which could lead to inefficiencies when different jobs have different bottlenecks . DRF instead allocates resources by equalizing each user’s **dominant resource share** – the fraction of the resource that the user is consuming most heavily . In effect, each user’s worst-case resource (CPU for CPU-intensive users, memory for memory-intensive users, etc.) is balanced so that no user’s dominant share exceeds another’s . The paper’s key contribution is an algorithm (and accompanying economic fairness interpretation) that achieves a **max-min fair allocation across multiple resources** . DRF also proves that, unlike naive extensions, it satisfies several **desirable fairness properties**: _sharing incentive_ (no one can do better by splitting the cluster equally), _strategy-proofness_ (users cannot lie about needs to get more), _envy-freeness_ (no user prefers another’s allocation), and _Pareto efficiency_ .
    
-   **Deployment scenario:** The DRF mechanism was implemented and evaluated in the **Mesos** cluster manager (see below) for scheduling datacenter resources . It is aimed at **cluster resource schedulers** that manage multiple resource types simultaneously – for example, assigning CPU cores, memory, and I/O bandwidth to different users or frameworks in a cloud cluster. DRF’s applicability is broad; it can be used in Hadoop or YARN fair schedulers, Mesos, or any multi-tenant resource allocator. Indeed, experiments showed DRF improved throughput and fairness compared to Hadoop’s older slot-based fair sharing .
    
-   **Fairness definition:** Under DRF, fairness means **each user’s dominant resource fraction is equal** in the final allocation . For example, if User A’s tasks need mostly CPU and User B’s need mostly RAM, DRF will allocate resources until the percentage of CPU A gets is roughly the same as the percentage of RAM B gets (their dominant shares). No user can increase their allocation of any resource without reducing someone else’s already smaller or equal dominant share – this is the multi-resource extension of max-min fairness . DRF ensures no user would benefit by demanding less of one resource to game the scheduler, and every user gets at least as much of each resource as they would under an equal split . In summary, fairness in DRF is an _equilibrium_ where each tenant has the same normalized “largest slice” of the resource pie, respecting everyone’s most needed resource.
    

  

## **Apache Mesos – Fair Sharing via Two-Level Scheduling (Hindman et al., 2011)**

-   **Main idea:** **Mesos** is a cluster resource manager that enables fine-grained sharing of a cluster among multiple _frameworks_ (e.g. Hadoop, Spark, MPI) . While Mesos’s core contribution is its two-level scheduling architecture (Mesos decides how many resources to offer each framework, and each framework’s scheduler picks tasks to launch) , fairness is a critical part of Mesos’s resource allocation policy. The Mesos master uses an **allocation module** to decide how to divide available resources among competing frameworks according to a configurable policy. Notably, the default policy implemented is **Dominant Resource Fairness**, which performs _fair sharing based on a generalization of max-min fairness for multiple resources_ . In practice, this means Mesos offers resources to frameworks in such a way that each framework gets an approximately equal dominant share of the cluster (respecting any weights). Mesos thereby ensures that no single framework (e.g., a big Hadoop job) can monopolize CPUs or memory across the cluster to the detriment of others.
    
-   **Deployment scenario:** Mesos operates in large-scale **datacenters** and was designed to run on clusters of tens of thousands of nodes . It is a _two-level_ scheduler – a central coordinator allocates resource quanta to frameworks, which then do their internal scheduling. The fairness mechanism in Mesos is applied at the **inter-framework level**: e.g., sharing a cluster between Hadoop and Spark. This was evaluated on scenarios where frameworks either get a static partition or use Mesos’s fair sharing; Mesos demonstrated better data locality and cluster utilization by letting frameworks “take turns” on each machine rather than static splits .
    
-   **Fairness definition:** Mesos defines fairness in terms of **share of resources allocated to each framework**. By default it implements _weighted fair sharing_ equivalent to DRF (each framework should receive resources such that their dominant usage is balanced) . For example, if two frameworks are running, each should get roughly 50% of each resource in the long run (or weighted percentages if configured unequally). The Mesos paper explicitly mentions an allocation module for fair sharing based on dominant shares . Thus, fairness means each framework gets its entitled fraction of the cluster, and frameworks with less demand can’t be overtaken by ones with larger appetites. Within each framework, Mesos leaves scheduling to the framework’s own scheduler – e.g., Hadoop’s internal fair scheduler might ensure per-job fairness – but Mesos ensures **per-tenant (per-framework) fairness** at the cluster level. In short, no framework is starved: Mesos’s policy offers roughly equal resources to all active frameworks over time .
    

  

## **Pisces: Per-Tenant Fairness in Cloud Storage (Shue et al., 2012)**

-   **Main idea:** **Pisces** is a system that provides **performance isolation and fairness in a multi-tenant storage service** (specifically, a distributed key-value store) . The goal is to ensure each tenant of a cloud storage system receives its fair share of throughput, even when tenants have different traffic patterns or hotspots. Pisces introduces a combination of techniques: **partition placement** algorithms to distribute data such that each tenant can get its share of each server’s capacity, and **distributed weighted fair queuing** to schedule requests on each storage node . Each tenant is assigned a weight corresponding to a fraction of the global service capacity (e.g. tenant A pays for 20% throughput, tenant B 10%, etc.). The system translates these global weights into local weights on each server that hosts that tenant’s data, and uses packet-style fair queuing to serve requests according to those weights . A tenant with unused share can’t block others – if one tenant is idle, others can use the free capacity (work-conserving behavior) . Pisces’s design thus spans _policy_ (global weight assignments and partitioning for fairness) and _mechanism_ (local scheduling via fair queueing).
    
-   **Deployment scenario:** Pisces targets **multi-tenant cloud storage systems** such as hosted NoSQL databases or object stores. It was evaluated on a prototype built on a distributed in-memory key-value store (similar to Membase) . In such systems, multiple tenants’ data partitions reside on the same storage nodes, causing contention for disk, CPU, and network. Pisces ensures system-wide fairness by coordinating decisions across all nodes. It assumes a data center environment where network bandwidth is ample (focusing on the storage node bottlenecks) . This work is a prime example of extending fairness beyond CPU scheduling to **throughput fairness in storage services**.
    
-   **Fairness definition:** Pisces defines fairness as **each tenant achieving its global share of the service’s capacity**. Formally, each tenant _t_ is given a weight _wt_ that represents its fraction of total throughput . The system guarantees that if all tenants are fully utilizing the service, each will receive roughly its weight’s proportion of requests/sec. For instance, with equal weights, two tenants should each get ~50% of the aggregate throughput . Importantly, Pisces provides a _minimum throughput guarantee_ (for tenants with reserved rates) while allowing bursts: _“the system ensures [the minimum rate], yet also allows users to exploit unused capacity”_ when others are idle . Fairness is enforced locally by weighted fair queuing on each server (preventing any tenant from exceeding its share on that node) , and ensured globally by adjusting weights per node to align with the tenant’s overall entitlement . In summary, a tenant cannot consume more than its fair share of any single server or the total cluster, and even under heavy load each tenant achieves **per-tenant fair throughput** (e.g., equal-share tenants each get ~100 k req/s in a 200 k req/s system) .
    

  

## **Hierarchical Dominant Resource Fairness (Bhattacharya et al., 2013)**

-   **Main idea:** This work extends the DRF concept to **hierarchical resource scheduling** scenarios often found in organizations . In practice, many cluster schedulers organize tenants in a hierarchy of queues or groups (for example, an organization might split cluster resources 60% to Dept. A and 40% to Dept. B, and within Dept. A allocate 70% to production jobs vs 30% testing) . Bhattacharya et al. identify challenges in applying multi-resource fairness in such hierarchies – naive methods could leave resources unallocated or starve some jobs . They propose **H-DRF (Hierarchical Dominant Resource Fairness)**, an algorithm that generalizes DRF to tree-structured groups of users . H-DRF ensures that **each node in the hierarchy (at any level) gets at least its fair share** of resources, regardless of demands elsewhere . This _hierarchical share guarantee_ means, for example, if a department is entitled to 40% of the cluster, H-DRF will allocate resources such that the sum of its users’ allocations is 40% (when there is enough demand), even if other departments have hungry users . Moreover, within that department, its internal users are allocated resources fairly (by DRF) respecting their weights. The algorithm avoids starvation and inefficiencies observed in prior Hadoop YARN schedulers, and is proven _group-strategy-proof_ (no collusion of users can game the allocation) .
    
-   **Deployment scenario:** H-DRF was implemented in **Hadoop YARN’s Capacity Scheduler** (an open-source scheduler that supports hierarchical queues) . The target scenario is a **multi-tenant cluster with organizational or priority hierarchies**, common in enterprise or cloud setups where resources are divided among business units, projects, or user groups. For example, a company’s cluster might reserve certain percentages for different teams; H-DRF operates to enforce those allocations fairly across multiple resource dimensions. The evaluation with Facebook’s production traces and cluster prototypes showed H-DRF could utilize resources better than the then-existing Hadoop schedulers (which were slot-based) and eliminate starvation in edge cases .
    
-   **Fairness definition:** Fairness in H-DRF is defined **recursively at each hierarchy level**. Each group or user has a weight (or quota) and should receive resources in proportion to that weight, _subject to the constraint that parent groups get their global share_. The **Hierarchical Share Guarantee** means a child group cannot steal resources from siblings if its parent’s share is satisfied . Practically, H-DRF ensures _no job or inner group will starve_: every leaf gets at least its min share through the hierarchy, and if a parent isn’t using its full allocation, only its siblings (not the entire cluster) can claim the slack . This preserves the intent of hierarchical policies. At the lowest level (leaf users/jobs), fairness behaves like DRF (dominant shares equalized among those siblings). Thus, H-DRF’s fairness notion is **per-group dominant resource fairness**: equalize dominant resource usage among entities _within each branch_ of the hierarchy, while enforcing each branch’s overall share of the cluster . This ensures both isolation across top-level groups and fairness among leaf users – a user cannot get more than its fair share for its role, and an entire department cannot collectively exceed its allotted percentage of resources.
    

  

## **Carbyne: Long-Term Fairness vs. Efficiency (Grandl et al., 2016)**

-   **Main idea:** **Carbyne** introduces an _altruistic scheduling_ approach that revisits fairness from a longer-term perspective . Many cluster schedulers (like DRF and others) focus on **instantaneous fairness** – trying to equalize allocations at every moment to ensure strict isolation . Carbyne argues that this short-term fairness can lead to suboptimal utilization without tangible long-term benefits . Instead, Carbyne allows jobs to **yield a portion of their resources** temporarily if those resources don’t help them finish any faster, and gives those resources to other jobs that can use them more effectively . In other words, a job will not hold resources idle just to maintain an instantaneous fair share. By doing this, Carbyne accumulates “leftover” resources which it reallocates to improve overall throughput and job completion times – all while ensuring that no job’s completion is delayed compared to a strictly fair schedule . The result is a scheduler that achieves nearly the same isolation as DRF (no one is significantly worse off) but with significantly better efficiency and faster average job completion (1.26× higher cluster throughput and 1.59× lower job time in experiments) .
    
-   **Deployment scenario:** Carbyne is designed for **multi-resource cluster schedulers** in datacenters, much like DRF, but especially in environments where jobs have varying resource usage patterns over time. For instance, big data or HPC clusters where some jobs may leave CPU cores idle while waiting for IO – Carbyne would temporarily give those idle cores to other jobs. The system was evaluated through simulations and a deployment, indicating it can be integrated into cluster managers. It doesn’t replace fairness policies but augments them by introducing a controlled leeway in allocation timing.
    
-   **Fairness definition:** Carbyne’s notion of fairness emphasizes **long-term fairness and performance isolation** rather than moment-to-moment equal sharing. It still guarantees that each user (or job) gets _at least_ what a fair scheduler (like DRF) would give them in terms of final allocation and completion time. In the Carbyne paper’s terms, it “closely approximates…DRF in terms of performance isolation” – meaning no job is significantly slowed down compared to DRF’s strict fairness. However, Carbyne relaxes **instantaneous fairness**: at any given instant, some jobs might have more than their “fair share” and others less, as long as those with less are not being delayed in completion. The fairness guarantee is thus shifted to the outcome (job finish times) rather than the exact allocation at each timeslice. In summary, Carbyne defines fairness such that _no tenant is worse off than they would be under strict fair sharing_, but tenants can be **altruistic** in donating unused resources. This new policy shows that allowing slight fairness rule-bending in the short term (when safe) can still meet fairness goals in the long term while improving efficiency .
    

  

## **Themis: Fairness for ML Clusters (Mahajan et al., 2020)**

-   **Main idea:** **Themis** tackles fairness in the context of shared GPU clusters for machine learning workloads, introducing a new fairness metric called **finish-time fairness** . ML training jobs have characteristics (gang-scheduled tasks, sensitivity to placement together on machines, variable run times) that can make traditional fair schedulers (like DRF or even Carbyne) “unfair” in practice . For example, DRF might equally allocate GPUs, but if one job’s tasks are split across servers (worse placement), it could progress much slower than another job with the same number of GPUs all on one server – violating the spirit of fairness . Themis defines fairness in terms of **job completion times**: each user’s job should finish in a shared cluster nearly as fast as it would if the user had a dedicated cluster slice. Specifically, Themis provides _sharing incentive_ – if _N_ users share a cluster of size _C_, no user’s job takes longer than it would on a private cluster of size _C/N_ . To achieve this, Themis uses a two-level scheduling approach with an auction: it periodically reallocates GPUs by asking jobs how far behind their ideal finish-time they are, then gives priority to those lagging (trading off some short-term efficiency for long-term fairness) . Over time, this yields an allocation that equalizes the _slowdown_ each job experiences due to sharing.
    
-   **Deployment scenario:** Themis is designed for **GPU clusters running parallel ML training jobs** (common in modern enterprises and research). These clusters are multi-tenant (different teams or users training models) and need to ensure fair access to expensive GPU resources. Themis was evaluated on traces from Microsoft’s production clusters and showed it can improve fairness (in terms of equalized finish times) by over 2× while also slightly improving utilization . It targets scenarios where existing fair schedulers fail to account for issues like gang scheduling (all-or-nothing GPU allocation for a job) and placement sensitivity.
    
-   **Fairness definition:** Themis defines fairness via the **Finish-Time Fairness** metric _r_, which is the ratio of a job’s runtime in the shared cluster to its runtime on an ideally fair dedicated share . Perfect fairness means _r = 1_ for all jobs (each finishes in the same time as if it had 1/_N_ of the cluster to itself). Themis’s goal is to minimize the maximum _r_ across all jobs , essentially ensuring no job is disproportionately slowed. This encapsulates _sharing incentive_ – no one is worse off by sharing – as well as **Pareto efficiency and envy-freeness** adapted to finish times (users would not prefer each other’s outcome) . In practice, Themis guarantees that each user’s performance is _no worse_ than their entitlement. If one job falls behind (has a higher _r_), Themis’ auction mechanism will allocate it more GPUs in the next rounds to catch up . Thus fairness is maintained in a long-term sense: all users experience similar slowdowns due to sharing, making the arrangement fair. This is a departure from resource-share equality; instead it ensures **fair sharing of performance** (each user gets equal relative performance as if the cluster were partitioned). Finish-time fairness extends the fairness concept to scenarios where equal resource slices might not translate to equal outcomes, focusing on equalizing outcomes instead.
    

  

## **Summary of Key Fairness Mechanisms (2010–2025)**

**Paper (Year)**

**Deployment Target**

**Fairness Notion**

**Core Contribution**

**Linux CFS** (2007)

OS process scheduler (single machine)

Equal CPU time for tasks (weighted by priority); essentially WFQ scheduling .

Introduced _completely fair_ CPU scheduling, giving each task an ideal fair share of the processor . Ensures no starvation in multi-process OS environments.

**Delay Scheduling** (2010)

Cluster batch scheduler (Hadoop MapReduce)

Max-min fair share of “slots” per job, with short-term compromises for data locality. Fairness measured over time (each job gets equal slot share) .

Simple scheduling tweak: if a fair-share turn can’t get local data, it yields briefly. Achieves near-optimal data locality _while preserving fairness_ in throughput .

**Dominant Resource Fairness** (2011)

Cluster resource managers (multi-resource)

Each user’s dominant resource fraction equalized across users (multi-resource max-min fairness) . Ensures sharing incentive, strategy-proofness, envy-freeness, Pareto efficiency .

Formulated multi-resource fair allocation rule (DRF). Extended fair sharing to CPU, memory, etc. simultaneously . Became basis for modern multi-resource schedulers (e.g. Mesos) with strong fairness guarantees.

**Apache Mesos** (2011)

Datacenter cluster manager (multi-framework)

Fair sharing among frameworks (often using DRF for multi-resources) . Each framework gets resources proportional to its share; no framework starves.

Developed two-level scheduling: central allocator offers resources fairly among frameworks, frameworks do internal scheduling . Demonstrated efficient, fair cluster sharing between diverse workloads (e.g., Hadoop, MPI).

**Pisces** (2012)

Distributed storage service (cloud DB)

Per-tenant weighted fair throughput. Each tenant gets at least its weight’s share of global request rate ; unused capacity is redistributed.

Achieved tenant isolation in a key-value store via global-local weight allocation and fair queuing . Ensured system-wide fairness in throughput and automatic load balancing of hot partitions.

**Hierarchical DRF** (2013)

Hierarchical cluster scheduler (YARN, etc.)

Hierarchical fairness: enforce fair shares at each level of org hierarchy. Guarantees each group its minimum share (no lower-level entity can steal it) ; within a group, use DRF among members.

Generalized DRF to support hierarchical resource pools . Solved starvation and inefficiency issues in prior hierarchical schedulers. Provided _hierarchical share guarantee_ and group strategy-proofness in multi-resource settings.

**Carbyne** (2016)

Cluster scheduler (multi-resource)

Long-term fairness (performance isolation) with short-term altruism. Users get at least their fair share over time, though instantaneous allocation may deviate . Fairness judged by final outcomes (no worse than strict fair scheduling) .

Introduced _altruistic scheduling_: jobs voluntarily yield unused resources, which are reallocated to boost efficiency without hurting any job’s completion time . Achieved similar fairness to DRF but with ~1.3× better throughput and faster job completions .

**Themis** (2020)

GPU cluster scheduler (ML training jobs)

Finish-time fairness: equalize each user’s slow-down compared to having a dedicated cluster slice . Ensures sharing incentive (no user’s job is slower than if cluster split equally) and preserves Pareto efficiency & envy-freeness for job finish times .

Defined a new fairness metric for gang-scheduled, placement-sensitive workloads. Used an auction-based scheduler to minimize max slowdown, giving all jobs comparable finish times . Greatly improved fairness for ML jobs (2× reduction in unfairness) while maintaining high GPU utilization .

Each of these works has advanced the state of fair resource sharing in multi-tenant systems. From OS-level CPU scheduling to multi-resource datacenter scheduling and domain-specific solutions, the evolution shows a trend toward richer fairness definitions – accounting not just for equal resource _allocation_, but for equal _benefit_ (e.g., finish times, throughput) and accommodating complex realities like hierarchical organizational policies and workload characteristics. Together, these research contributions form a foundation for designing schedulers that ensure fairness — so that cloud users and applications can share infrastructure without disadvantage, and system resources are utilized efficiently without sacrificing equity.

  

**Sources:** The summaries above draw on the original research papers and their findings, with key points and definitions referenced from those works . Each cited study has significantly influenced how modern operating systems and cloud platforms implement fair scheduling.




## Fairness Taxonomy (Single-Resource Setting)

This section categorizes prior systems and research works by **what notion of fairness they primarily enforce**.  
Each column corresponds to a distinct fairness definition.  
Symbols indicate how central that notion is to the design.

### Symbol Legend

- **✅ Primary** — this is the main fairness goal the system is designed to enforce  
- **◐ Secondary** — supported implicitly or approximately, but not the main objective  
- **❌ Not targeted** — this notion is not a design goal of the system  

---

## Fairness Categories Explained

### 1. Instantaneous Share Fairness
**What is fair:**  
The *current allocation* of resources at any instant in time.

**Formal intuition:**  
At any time \( t \), no job may exceed its entitled fraction:
$$s_i(t) \le w_i \cdot C$$

**Interpretation:**  
- Prevents monopolization
- Ensures no instantaneous envy
- Purely spatial; ignores history

**Limitations:**  
- Does not ensure equal total service
- Does not bound waiting time
- Completion times may be highly unfair

**Typical use:**  
Capacity caps, quotas, safety isolation.

---

### 2. Service-Time Fairness (Cumulative Fairness)
**What is fair:**  
The *total amount of resource-time* a job receives over long horizons.

**Formal intuition:**  
$$
\int_0^T s_i(t)\,dt \approx w_i \cdot \int_0^T C(t)\,dt
$$

**Interpretation:**  
- “Everyone eventually gets their fair share”
- Time is explicit, outcomes are not

**Strengths:**  
- Starvation freedom (eventual)
- Envy-freeness in the limit

**Limitations:**  
- Long unfair prefixes are possible
- Equal service ≠ equal progress for DAG jobs

**Typical use:**  
Cluster schedulers (Hadoop, Mesos, YARN).

---

### 3. Bounded-Lag Service Fairness
**What is fair:**  
Service-time fairness enforced **at all prefixes**, up to a bounded deviation.

**Formal intuition:**  
$$
\left| \int_0^t s_i(\tau)d\tau - w_i \cdot \int_0^t C(\tau)d\tau \right| \le \Delta
$$

**Interpretation:**  
- “No job can fall too far behind”
- Strong temporal fairness

**Strengths:**  
- Predictable progress
- Strong starvation avoidance

**Limitations:**  
- Requires fine granularity or preemption
- Hard with non-preemptive gang stages

**Typical use:**  
OS schedulers (Linux CFS), WFQ-like systems.

---

### 4. Rate-Based / Virtualization Fairness
**What is fair:**  
The *service rate* experienced by each job.

**Formal intuition:**  
$$
\frac{d}{dt}\text{Progress}_i(t) = w_i \cdot C
$$

**Interpretation:**  
- Jobs run on **logical resources**
- Excess demand causes slowdown, not blocking
- Spatial analogue of CPU virtualization

**Strengths:**  
- Intuitive user model (“I get 50% of the system”)
- Natural for malleable, non-preemptive jobs

**Limitations:**  
- Does not optimize completion time
- Requires malleability to be feasible

**Typical use:**  
Storage systems (Pisces), virtualization-inspired designs.

---

### 5. Outcome / Finish-Time Fairness
**What is fair:**  
The *job outcome*, typically completion time or slowdown.

**Formal intuition:**  
$$
\text{Slowdown}_i = \frac{C_i}{C_i^{\text{solo}}}
\quad\text{(fair if roughly equal)}
$$

**Interpretation:**  
- “Everyone is slowed down equally by sharing”
- Time is the primary fairness currency

**Strengths:**  
- Matches user-perceived fairness
- Handles DAGs and critical paths well

**Limitations:**  
- Resource allocations may look unfair
- Requires estimating solo runtimes

**Typical use:**  
Modern ML and cluster systems (Carbyne, Themis).

---

## Assignment of Prior Works to Fairness Categories

| Work | Instantaneous | Service-time | Bounded-lag | Rate-based | Outcome |
|------|---------------|--------------|-------------|------------|---------|
| Linux CFS | ◐ | ✅ | ✅ | ◐ | ❌ |
| Hadoop Fair Scheduler | ◐ | ✅ | ❌ | ❌ | ❌ |
| Delay Scheduling | ◐ | ✅ | ◐ | ❌ | ❌ |
| DRF (1D) | ❌ | ✅ | ❌ | ❌ | ❌ |
| Apache Mesos (allocator) | ◐ | ✅ | ❌ | ❌ | ❌ |
| YARN Capacity / H-DRF | ◐ | ✅ | ❌ | ❌ | ❌ |
| Pisces | ◐ | ◐ | ❌ | ✅ | ❌ |
| Carbyne | ❌ | ◐ | ❌ | ❌ | ✅ |
| Themis | ❌ | ◐ | ❌ | ❌ | ✅ |
| Moldable job scheduling (HPC theory) | ❌ | ❌ | ❌ | ✅ | ❌ |

---

## Key Takeaway

These systems are **not incremental improvements of one another**.  
They enforce *different fairness contracts*:

- **Schedulers** focus on *who has consumed how much*  
- **OS schedulers** focus on *who is falling behind*  
- **Virtualization systems** focus on *what rate you experience*  
- **Modern ML systems** focus on *how slow you feel*

Your proposed model aligns most strongly with **rate-based / virtualization fairness**, a category that is conceptually clean but underexplored in cluster scheduling literature.


> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTcyMTE1NDg5MiwxMTc0Mjc3MzA2LC0xOT
Q0NTAzMzksMjA5MTk4Mzc2NSwtMzY5NTQ0NTYyLC0yMDU3NzIx
NTgyLDUzNDkwOTg0NCwzNjUxNDcxNzBdfQ==
-->