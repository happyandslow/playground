

# **Fairness in Multi-Tenant Resource Sharing: Key Research (2010â€“2025)**

  

## **Introduction**

  

In multi-tenant computing environments â€“ whether in cloud datacenters or traditional operating systems â€“ **fairness** is a central concern. When multiple users or workloads share CPUs, memory, I/O, or other resources, the system must allocate these resources such that no tenant is unfairly starved or advantaged. Broadly, fairness can be enforced through **algorithmic mechanisms** (scheduling algorithms that split resources in real-time) and **policy-level definitions** (high-level rules or share guarantees configured for users or groups). A classic baseline is **max-min fairness**, which allocates resources such that any increase in one userâ€™s allocation would force a decrease for another user with equal or lesser allocation . In practice, simple equal sharing is often refined by weights or multi-resource considerations to reflect priorities and diverse demands. Over the past 15 years, numerous influential works have defined fairness more rigorously and designed new schedulers to achieve it. Below, we survey several well-cited research contributions, summarizing each paperâ€™s main ideas, target deployment scenario, and definition of fairness.

  

## **OS-Level Fair Scheduling â€“ Linux CFS (2007)**

-   **Main idea:** The Linux **Completely Fair Scheduler (CFS)** exemplifies OS-level fairness by aiming to give each process an equal share of CPU time. CFS is essentially an implementation of _weighted fair queueing (WFQ)_ for CPU scheduling, dividing CPU cycles among runnable threads in proportion to their weights (priority) . This model â€œ_does away with_â€ fixed time-slice quotas of earlier schedulers and instead continuously balances execution time so that no task runs more than its fair share .
    
-   **Deployment scenario:** CFS became the default scheduler in Linux 2.6.23+ and is used in traditional operating systems on single machines . It manages CPU time across all processes (or groups of processes) on a core, maintaining per-core run queues and selecting the next task to run based on who has used the least CPU so far (tracked via a virtual runtime) . This design keeps the scheduler _work-conserving_ (no CPU sits idle if tasks are waiting) and scales to multi-core systems with per-CPU queues.
    
-   **Fairness definition:** CFS defines fairness as **proportional equal-progress** for tasks. In an ideal sense, if _N_ tasks are running with equal weight, each should get ~1/_N_ of the processor. Threads with higher weight (lower â€œnicenessâ€) simply get a larger fraction proportionally . By continuously picking the task with the smallest accumulated runtime to run next, CFS ensures that over the long term each taskâ€™s CPU time converges to its fair share . This prevents any single process from hogging the CPU and provides _isolation_: even if one task would use 100% CPU, it cannot exceed its allotted share while others are runnable.
    

  

## **Delay Scheduling for Cluster Fairness (Zaharia et al., 2010)**

-   **Main idea:** _Delay Scheduling_ was introduced to reconcile fairness with data locality in cluster computing (specifically Hadoop MapReduce) . In a multi-user Hadoop cluster, a **fair scheduler** tries to give each job an equal share of running tasks (â€œslotsâ€). However, strictly enforcing fairness can hurt performance if a jobâ€™s tasks are forced to run on nodes without the needed data. Zaharia et al. propose a simple tweak: if the job that should be scheduled next (to maintain fairness) cannot launch a task on a local node, it **waits briefly**, allowing another job to use the slot . This short delay lets other tasks run (improving cluster utilization and locality) without significantly violating fairness over time.
    
-   **Deployment scenario:** Delay scheduling was implemented in Facebookâ€™s 600-node Hadoop cluster as part of a fair share scheduler . It targets **datacenter batch processing frameworks** where jobs are divisible into many tasks. The mechanism is used within a single-framework scheduler (Hadoopâ€™s Fair Scheduler) to balance multiple usersâ€™ jobs on a shared cluster.
    
-   **Fairness definition:** The underlying fairness policy here is **slot-based max-min fairness** â€“ each user (or job) should get an approximately equal number of task slots over the cluster. Delay scheduling ensures this _over the long run_ while yielding momentarily on strict ordering to improve data locality. Crucially, the algorithm was shown to â€œincrease throughput by up to 2Ã— **while preserving fairness**â€ â€“ i.e. jobs still receive equal shares of runtime on average . Fairness is measured by each jobâ€™s share of executed tasks: delay scheduling doesnâ€™t change each jobâ€™s overall share; it only postpones a job for a few seconds if it canâ€™t get a local slot, thereby achieving **nearly optimal locality without sacrificing fairness** .
    

  

## **Dominant Resource Fairness (Ghodsi et al., 2011)**

-   **Main idea:** **Dominant Resource Fairness (DRF)** is a landmark framework that generalizes fair sharing to environments with _multiple resource types_ (CPU, memory, etc.) . Ghodsi et al. observe that traditional fair schedulers reduced multi-resource allocation to a single dimension (e.g. â€œslotsâ€ with fixed CPU and RAM), which could lead to inefficiencies when different jobs have different bottlenecks . DRF instead allocates resources by equalizing each userâ€™s **dominant resource share** â€“ the fraction of the resource that the user is consuming most heavily . In effect, each userâ€™s worst-case resource (CPU for CPU-intensive users, memory for memory-intensive users, etc.) is balanced so that no userâ€™s dominant share exceeds anotherâ€™s . The paperâ€™s key contribution is an algorithm (and accompanying economic fairness interpretation) that achieves a **max-min fair allocation across multiple resources** . DRF also proves that, unlike naive extensions, it satisfies several **desirable fairness properties**: _sharing incentive_ (no one can do better by splitting the cluster equally), _strategy-proofness_ (users cannot lie about needs to get more), _envy-freeness_ (no user prefers anotherâ€™s allocation), and _Pareto efficiency_ .
    
-   **Deployment scenario:** The DRF mechanism was implemented and evaluated in the **Mesos** cluster manager (see below) for scheduling datacenter resources . It is aimed at **cluster resource schedulers** that manage multiple resource types simultaneously â€“ for example, assigning CPU cores, memory, and I/O bandwidth to different users or frameworks in a cloud cluster. DRFâ€™s applicability is broad; it can be used in Hadoop or YARN fair schedulers, Mesos, or any multi-tenant resource allocator. Indeed, experiments showed DRF improved throughput and fairness compared to Hadoopâ€™s older slot-based fair sharing .
    
-   **Fairness definition:** Under DRF, fairness means **each userâ€™s dominant resource fraction is equal** in the final allocation . For example, if User Aâ€™s tasks need mostly CPU and User Bâ€™s need mostly RAM, DRF will allocate resources until the percentage of CPU A gets is roughly the same as the percentage of RAM B gets (their dominant shares). No user can increase their allocation of any resource without reducing someone elseâ€™s already smaller or equal dominant share â€“ this is the multi-resource extension of max-min fairness . DRF ensures no user would benefit by demanding less of one resource to game the scheduler, and every user gets at least as much of each resource as they would under an equal split . In summary, fairness in DRF is an _equilibrium_ where each tenant has the same normalized â€œlargest sliceâ€ of the resource pie, respecting everyoneâ€™s most needed resource.
    

  

## **Apache Mesos â€“ Fair Sharing via Two-Level Scheduling (Hindman et al., 2011)**

-   **Main idea:** **Mesos** is a cluster resource manager that enables fine-grained sharing of a cluster among multiple _frameworks_ (e.g. Hadoop, Spark, MPI) . While Mesosâ€™s core contribution is its two-level scheduling architecture (Mesos decides how many resources to offer each framework, and each frameworkâ€™s scheduler picks tasks to launch) , fairness is a critical part of Mesosâ€™s resource allocation policy. The Mesos master uses an **allocation module** to decide how to divide available resources among competing frameworks according to a configurable policy. Notably, the default policy implemented is **Dominant Resource Fairness**, which performs _fair sharing based on a generalization of max-min fairness for multiple resources_ . In practice, this means Mesos offers resources to frameworks in such a way that each framework gets an approximately equal dominant share of the cluster (respecting any weights). Mesos thereby ensures that no single framework (e.g., a big Hadoop job) can monopolize CPUs or memory across the cluster to the detriment of others.
    
-   **Deployment scenario:** Mesos operates in large-scale **datacenters** and was designed to run on clusters of tens of thousands of nodes . It is a _two-level_ scheduler â€“ a central coordinator allocates resource quanta to frameworks, which then do their internal scheduling. The fairness mechanism in Mesos is applied at the **inter-framework level**: e.g., sharing a cluster between Hadoop and Spark. This was evaluated on scenarios where frameworks either get a static partition or use Mesosâ€™s fair sharing; Mesos demonstrated better data locality and cluster utilization by letting frameworks â€œtake turnsâ€ on each machine rather than static splits .
    
-   **Fairness definition:** Mesos defines fairness in terms of **share of resources allocated to each framework**. By default it implements _weighted fair sharing_ equivalent to DRF (each framework should receive resources such that their dominant usage is balanced) . For example, if two frameworks are running, each should get roughly 50% of each resource in the long run (or weighted percentages if configured unequally). The Mesos paper explicitly mentions an allocation module for fair sharing based on dominant shares . Thus, fairness means each framework gets its entitled fraction of the cluster, and frameworks with less demand canâ€™t be overtaken by ones with larger appetites. Within each framework, Mesos leaves scheduling to the frameworkâ€™s own scheduler â€“ e.g., Hadoopâ€™s internal fair scheduler might ensure per-job fairness â€“ but Mesos ensures **per-tenant (per-framework) fairness** at the cluster level. In short, no framework is starved: Mesosâ€™s policy offers roughly equal resources to all active frameworks over time .
    

  

## **Pisces: Per-Tenant Fairness in Cloud Storage (Shue et al., 2012)**

-   **Main idea:** **Pisces** is a system that provides **performance isolation and fairness in a multi-tenant storage service** (specifically, a distributed key-value store) . The goal is to ensure each tenant of a cloud storage system receives its fair share of throughput, even when tenants have different traffic patterns or hotspots. Pisces introduces a combination of techniques: **partition placement** algorithms to distribute data such that each tenant can get its share of each serverâ€™s capacity, and **distributed weighted fair queuing** to schedule requests on each storage node . Each tenant is assigned a weight corresponding to a fraction of the global service capacity (e.g. tenant A pays for 20% throughput, tenant B 10%, etc.). The system translates these global weights into local weights on each server that hosts that tenantâ€™s data, and uses packet-style fair queuing to serve requests according to those weights . A tenant with unused share canâ€™t block others â€“ if one tenant is idle, others can use the free capacity (work-conserving behavior) . Piscesâ€™s design thus spans _policy_ (global weight assignments and partitioning for fairness) and _mechanism_ (local scheduling via fair queueing).
    
-   **Deployment scenario:** Pisces targets **multi-tenant cloud storage systems** such as hosted NoSQL databases or object stores. It was evaluated on a prototype built on a distributed in-memory key-value store (similar to Membase) . In such systems, multiple tenantsâ€™ data partitions reside on the same storage nodes, causing contention for disk, CPU, and network. Pisces ensures system-wide fairness by coordinating decisions across all nodes. It assumes a data center environment where network bandwidth is ample (focusing on the storage node bottlenecks) . This work is a prime example of extending fairness beyond CPU scheduling to **throughput fairness in storage services**.
    
-   **Fairness definition:** Pisces defines fairness as **each tenant achieving its global share of the serviceâ€™s capacity**. Formally, each tenant _t_ is given a weight _wt_ that represents its fraction of total throughput . The system guarantees that if all tenants are fully utilizing the service, each will receive roughly its weightâ€™s proportion of requests/sec. For instance, with equal weights, two tenants should each get ~50% of the aggregate throughput . Importantly, Pisces provides a _minimum throughput guarantee_ (for tenants with reserved rates) while allowing bursts: _â€œthe system ensures [the minimum rate], yet also allows users to exploit unused capacityâ€_ when others are idle . Fairness is enforced locally by weighted fair queuing on each server (preventing any tenant from exceeding its share on that node) , and ensured globally by adjusting weights per node to align with the tenantâ€™s overall entitlement . In summary, a tenant cannot consume more than its fair share of any single server or the total cluster, and even under heavy load each tenant achieves **per-tenant fair throughput** (e.g., equal-share tenants each get ~100 k req/s in a 200 k req/s system) .
    

  

## **Hierarchical Dominant Resource Fairness (Bhattacharya et al., 2013)**

-   **Main idea:** This work extends the DRF concept to **hierarchical resource scheduling** scenarios often found in organizations . In practice, many cluster schedulers organize tenants in a hierarchy of queues or groups (for example, an organization might split cluster resources 60% to Dept. A and 40% to Dept. B, and within Dept. A allocate 70% to production jobs vs 30% testing) . Bhattacharya et al. identify challenges in applying multi-resource fairness in such hierarchies â€“ naive methods could leave resources unallocated or starve some jobs . They propose **H-DRF (Hierarchical Dominant Resource Fairness)**, an algorithm that generalizes DRF to tree-structured groups of users . H-DRF ensures that **each node in the hierarchy (at any level) gets at least its fair share** of resources, regardless of demands elsewhere . This _hierarchical share guarantee_ means, for example, if a department is entitled to 40% of the cluster, H-DRF will allocate resources such that the sum of its usersâ€™ allocations is 40% (when there is enough demand), even if other departments have hungry users . Moreover, within that department, its internal users are allocated resources fairly (by DRF) respecting their weights. The algorithm avoids starvation and inefficiencies observed in prior Hadoop YARN schedulers, and is proven _group-strategy-proof_ (no collusion of users can game the allocation) .
    
-   **Deployment scenario:** H-DRF was implemented in **Hadoop YARNâ€™s Capacity Scheduler** (an open-source scheduler that supports hierarchical queues) . The target scenario is a **multi-tenant cluster with organizational or priority hierarchies**, common in enterprise or cloud setups where resources are divided among business units, projects, or user groups. For example, a companyâ€™s cluster might reserve certain percentages for different teams; H-DRF operates to enforce those allocations fairly across multiple resource dimensions. The evaluation with Facebookâ€™s production traces and cluster prototypes showed H-DRF could utilize resources better than the then-existing Hadoop schedulers (which were slot-based) and eliminate starvation in edge cases .
    
-   **Fairness definition:** Fairness in H-DRF is defined **recursively at each hierarchy level**. Each group or user has a weight (or quota) and should receive resources in proportion to that weight, _subject to the constraint that parent groups get their global share_. The **Hierarchical Share Guarantee** means a child group cannot steal resources from siblings if its parentâ€™s share is satisfied . Practically, H-DRF ensures _no job or inner group will starve_: every leaf gets at least its min share through the hierarchy, and if a parent isnâ€™t using its full allocation, only its siblings (not the entire cluster) can claim the slack . This preserves the intent of hierarchical policies. At the lowest level (leaf users/jobs), fairness behaves like DRF (dominant shares equalized among those siblings). Thus, H-DRFâ€™s fairness notion is **per-group dominant resource fairness**: equalize dominant resource usage among entities _within each branch_ of the hierarchy, while enforcing each branchâ€™s overall share of the cluster . This ensures both isolation across top-level groups and fairness among leaf users â€“ a user cannot get more than its fair share for its role, and an entire department cannot collectively exceed its allotted percentage of resources.
    

  

## **Carbyne: Long-Term Fairness vs. Efficiency (Grandl et al., 2016)**

-   **Main idea:** **Carbyne** introduces an _altruistic scheduling_ approach that revisits fairness from a longer-term perspective . Many cluster schedulers (like DRF and others) focus on **instantaneous fairness** â€“ trying to equalize allocations at every moment to ensure strict isolation . Carbyne argues that this short-term fairness can lead to suboptimal utilization without tangible long-term benefits . Instead, Carbyne allows jobs to **yield a portion of their resources** temporarily if those resources donâ€™t help them finish any faster, and gives those resources to other jobs that can use them more effectively . In other words, a job will not hold resources idle just to maintain an instantaneous fair share. By doing this, Carbyne accumulates â€œleftoverâ€ resources which it reallocates to improve overall throughput and job completion times â€“ all while ensuring that no jobâ€™s completion is delayed compared to a strictly fair schedule . The result is a scheduler that achieves nearly the same isolation as DRF (no one is significantly worse off) but with significantly better efficiency and faster average job completion (1.26Ã— higher cluster throughput and 1.59Ã— lower job time in experiments) .
    
-   **Deployment scenario:** Carbyne is designed for **multi-resource cluster schedulers** in datacenters, much like DRF, but especially in environments where jobs have varying resource usage patterns over time. For instance, big data or HPC clusters where some jobs may leave CPU cores idle while waiting for IO â€“ Carbyne would temporarily give those idle cores to other jobs. The system was evaluated through simulations and a deployment, indicating it can be integrated into cluster managers. It doesnâ€™t replace fairness policies but augments them by introducing a controlled leeway in allocation timing.
    
-   **Fairness definition:** Carbyneâ€™s notion of fairness emphasizes **long-term fairness and performance isolation** rather than moment-to-moment equal sharing. It still guarantees that each user (or job) gets _at least_ what a fair scheduler (like DRF) would give them in terms of final allocation and completion time. In the Carbyne paperâ€™s terms, it â€œclosely approximatesâ€¦DRF in terms of performance isolationâ€ â€“ meaning no job is significantly slowed down compared to DRFâ€™s strict fairness. However, Carbyne relaxes **instantaneous fairness**: at any given instant, some jobs might have more than their â€œfair shareâ€ and others less, as long as those with less are not being delayed in completion. The fairness guarantee is thus shifted to the outcome (job finish times) rather than the exact allocation at each timeslice. In summary, Carbyne defines fairness such that _no tenant is worse off than they would be under strict fair sharing_, but tenants can be **altruistic** in donating unused resources. This new policy shows that allowing slight fairness rule-bending in the short term (when safe) can still meet fairness goals in the long term while improving efficiency .
    

  

## **Themis: Fairness for ML Clusters (Mahajan et al., 2020)**

-   **Main idea:** **Themis** tackles fairness in the context of shared GPU clusters for machine learning workloads, introducing a new fairness metric called **finish-time fairness** . ML training jobs have characteristics (gang-scheduled tasks, sensitivity to placement together on machines, variable run times) that can make traditional fair schedulers (like DRF or even Carbyne) â€œunfairâ€ in practice . For example, DRF might equally allocate GPUs, but if one jobâ€™s tasks are split across servers (worse placement), it could progress much slower than another job with the same number of GPUs all on one server â€“ violating the spirit of fairness . Themis defines fairness in terms of **job completion times**: each userâ€™s job should finish in a shared cluster nearly as fast as it would if the user had a dedicated cluster slice. Specifically, Themis provides _sharing incentive_ â€“ if _N_ users share a cluster of size _C_, no userâ€™s job takes longer than it would on a private cluster of size _C/N_ . To achieve this, Themis uses a two-level scheduling approach with an auction: it periodically reallocates GPUs by asking jobs how far behind their ideal finish-time they are, then gives priority to those lagging (trading off some short-term efficiency for long-term fairness) . Over time, this yields an allocation that equalizes the _slowdown_ each job experiences due to sharing.
    
-   **Deployment scenario:** Themis is designed for **GPU clusters running parallel ML training jobs** (common in modern enterprises and research). These clusters are multi-tenant (different teams or users training models) and need to ensure fair access to expensive GPU resources. Themis was evaluated on traces from Microsoftâ€™s production clusters and showed it can improve fairness (in terms of equalized finish times) by over 2Ã— while also slightly improving utilization . It targets scenarios where existing fair schedulers fail to account for issues like gang scheduling (all-or-nothing GPU allocation for a job) and placement sensitivity.
    
-   **Fairness definition:** Themis defines fairness via the **Finish-Time Fairness** metric _r_, which is the ratio of a jobâ€™s runtime in the shared cluster to its runtime on an ideally fair dedicated share . Perfect fairness means _r = 1_ for all jobs (each finishes in the same time as if it had 1/_N_ of the cluster to itself). Themisâ€™s goal is to minimize the maximum _r_ across all jobs , essentially ensuring no job is disproportionately slowed. This encapsulates _sharing incentive_ â€“ no one is worse off by sharing â€“ as well as **Pareto efficiency and envy-freeness** adapted to finish times (users would not prefer each otherâ€™s outcome) . In practice, Themis guarantees that each userâ€™s performance is _no worse_ than their entitlement. If one job falls behind (has a higher _r_), Themisâ€™ auction mechanism will allocate it more GPUs in the next rounds to catch up . Thus fairness is maintained in a long-term sense: all users experience similar slowdowns due to sharing, making the arrangement fair. This is a departure from resource-share equality; instead it ensures **fair sharing of performance** (each user gets equal relative performance as if the cluster were partitioned). Finish-time fairness extends the fairness concept to scenarios where equal resource slices might not translate to equal outcomes, focusing on equalizing outcomes instead.
    

  

## **Summary of Key Fairness Mechanisms (2010â€“2025)**

**Paper (Year)**

**Deployment Target**

**Fairness Notion**

**Core Contribution**

**Linux CFS** (2007)

OS process scheduler (single machine)

Equal CPU time for tasks (weighted by priority); essentially WFQ scheduling .

Introduced _completely fair_ CPU scheduling, giving each task an ideal fair share of the processor . Ensures no starvation in multi-process OS environments.

**Delay Scheduling** (2010)

Cluster batch scheduler (Hadoop MapReduce)

Max-min fair share of â€œslotsâ€ per job, with short-term compromises for data locality. Fairness measured over time (each job gets equal slot share) .

Simple scheduling tweak: if a fair-share turn canâ€™t get local data, it yields briefly. Achieves near-optimal data locality _while preserving fairness_ in throughput .

**Dominant Resource Fairness** (2011)

Cluster resource managers (multi-resource)

Each userâ€™s dominant resource fraction equalized across users (multi-resource max-min fairness) . Ensures sharing incentive, strategy-proofness, envy-freeness, Pareto efficiency .

Formulated multi-resource fair allocation rule (DRF). Extended fair sharing to CPU, memory, etc. simultaneously . Became basis for modern multi-resource schedulers (e.g. Mesos) with strong fairness guarantees.

**Apache Mesos** (2011)

Datacenter cluster manager (multi-framework)

Fair sharing among frameworks (often using DRF for multi-resources) . Each framework gets resources proportional to its share; no framework starves.

Developed two-level scheduling: central allocator offers resources fairly among frameworks, frameworks do internal scheduling . Demonstrated efficient, fair cluster sharing between diverse workloads (e.g., Hadoop, MPI).

**Pisces** (2012)

Distributed storage service (cloud DB)

Per-tenant weighted fair throughput. Each tenant gets at least its weightâ€™s share of global request rate ; unused capacity is redistributed.

Achieved tenant isolation in a key-value store via global-local weight allocation and fair queuing . Ensured system-wide fairness in throughput and automatic load balancing of hot partitions.

**Hierarchical DRF** (2013)

Hierarchical cluster scheduler (YARN, etc.)

Hierarchical fairness: enforce fair shares at each level of org hierarchy. Guarantees each group its minimum share (no lower-level entity can steal it) ; within a group, use DRF among members.

Generalized DRF to support hierarchical resource pools . Solved starvation and inefficiency issues in prior hierarchical schedulers. Provided _hierarchical share guarantee_ and group strategy-proofness in multi-resource settings.

**Carbyne** (2016)

Cluster scheduler (multi-resource)

Long-term fairness (performance isolation) with short-term altruism. Users get at least their fair share over time, though instantaneous allocation may deviate . Fairness judged by final outcomes (no worse than strict fair scheduling) .

Introduced _altruistic scheduling_: jobs voluntarily yield unused resources, which are reallocated to boost efficiency without hurting any jobâ€™s completion time . Achieved similar fairness to DRF but with ~1.3Ã— better throughput and faster job completions .

**Themis** (2020)

GPU cluster scheduler (ML training jobs)

Finish-time fairness: equalize each userâ€™s slow-down compared to having a dedicated cluster slice . Ensures sharing incentive (no userâ€™s job is slower than if cluster split equally) and preserves Pareto efficiency & envy-freeness for job finish times .

Defined a new fairness metric for gang-scheduled, placement-sensitive workloads. Used an auction-based scheduler to minimize max slowdown, giving all jobs comparable finish times . Greatly improved fairness for ML jobs (2Ã— reduction in unfairness) while maintaining high GPU utilization .

Each of these works has advanced the state of fair resource sharing in multi-tenant systems. From OS-level CPU scheduling to multi-resource datacenter scheduling and domain-specific solutions, the evolution shows a trend toward richer fairness definitions â€“ accounting not just for equal resource _allocation_, but for equal _benefit_ (e.g., finish times, throughput) and accommodating complex realities like hierarchical organizational policies and workload characteristics. Together, these research contributions form a foundation for designing schedulers that ensure fairness â€” so that cloud users and applications can share infrastructure without disadvantage, and system resources are utilized efficiently without sacrificing equity.

  

**Sources:** The summaries above draw on the original research papers and their findings, with key points and definitions referenced from those works . Each cited study has significantly influenced how modern operating systems and cloud platforms implement fair scheduling.






Great â€” below is a **clean assignment** of the **specific works we discussed earlier** into the **fairness-definition categories** I laid out, **under a single-resource setting**. Iâ€™ll be explicit about _primary_ vs _secondary_ notions, because many systems mix them.

----------

## **Fairness Categories (recap, single resource)**

1.  **Instantaneous share fairness**
    
    Fairness over s_i(t)
    
2.  **Service (resource-time) fairness**
    
    Fairness over \int s_i(t)\,dt
    
3.  **Bounded-lag service fairness**
    
    Fairness over _prefix_ integrals
    
4.  **Rate-based / virtualization fairness**
    
    Fairness over _service rate_ (logical resources)
    
5.  **Outcome / finish-time fairness**
    
    Fairness over completion / slowdown
    

----------

## **Assignment of Works to Fairness Categories**

  

### **1. Linux CFS (Completely Fair Scheduler)**

  

**Primary category**

-   âœ… **Bounded-lag service fairness**
    

  

**Why**

-   Tracks _virtual runtime_
    
-   Always schedules the task with the least accumulated service
    
-   Guarantees bounded deviation from ideal share
    

  

**Secondary**

-   Service fairness (long-run)
    
-   Weak instantaneous fairness (only implicitly)
    

  

**Does NOT do**

-   Outcome fairness
    
-   Explicit rate virtualization (though users perceive it that way)
    

  

ðŸ“Œ **Canonical example** of bounded-lag fairness.

----------

### **2. Hadoop Fair Scheduler**

  

**Primary category**

-   âœ… **Service (resource-time) fairness**
    

  

**Why**

-   Fairness = equal slot-time over long horizons
    
-   Uses cumulative usage, not strict prefix bounds
    

  

**Secondary**

-   Weak instantaneous fairness (heuristics)
    
-   Some starvation avoidance
    

  

**Does NOT do**

-   Bounded-lag fairness
    
-   Outcome fairness
    
-   Rate virtualization
    

  

ðŸ“Œ **Classic long-run fairness**, weak temporal guarantees.

----------

### **3. Delay Scheduling (Zaharia et al.)**

  

**Primary category**

-   âœ… **Service (resource-time) fairness**
    

  

**Why**

-   Explicitly preserves fair-share slot-time
    
-   Temporary violations allowed for locality
    

  

**Secondary**

-   Slight bounded-lag improvement (via short delays)
    

  

**Does NOT do**

-   Outcome fairness
    
-   Rate virtualization
    

  

ðŸ“Œ **Service fairness with locality-aware slack**, not a new fairness definition.

----------

### **4. Dominant Resource Fairness (DRF)**

  

_(single-resource reduction)_

  

**Primary category**

-   âœ… **Service (resource-time) fairness**
    

  

**Why**

-   Reduces to classic maxâ€“min fairness in 1D
    
-   Equalizes cumulative resource shares
    

  

**Secondary**

-   Envy-freeness
    
-   Sharing incentive
    

  

**Does NOT do**

-   Bounded-lag fairness
    
-   Outcome fairness
    
-   Rate virtualization
    

  

ðŸ“Œ **Axiomatic generalization** of service fairness.

----------

### **5. Apache Mesos (allocator level)**

  

**Primary category**

-   âœ… **Service (resource-time) fairness**
    

  

**Why**

-   Allocator gives offers to lowest-share framework
    
-   Fairness enforced over time, not instantaneously
    

  

**Secondary**

-   Instantaneous caps (via quotas)
    

  

**Does NOT do**

-   Bounded-lag guarantees
    
-   Outcome fairness
    
-   Rate virtualization
    

  

ðŸ“Œ **Framework-level service fairness**, coarse granularity.

----------

### **6. YARN Capacity Scheduler / Hierarchical DRF**

  

**Primary category**

-   âœ… **Service (resource-time) fairness**
    

  

**Why**

-   Guarantees each queue its configured share over time
    
-   Hierarchical allocation preserves long-run shares
    

  

**Secondary**

-   Starvation avoidance
    
-   Instantaneous caps at queue boundaries
    

  

**Does NOT do**

-   Bounded-lag fairness
    
-   Outcome fairness
    

  

ðŸ“Œ **Policy-constrained service fairness**.

----------

### **7. Pisces (multi-tenant storage fairness)**

  

**Primary category**

-   âœ… **Rate-based / virtualization fairness**
    

  

**Why**

-   Tenants receive a guaranteed _throughput rate_
    
-   Excess demand slows the tenant rather than blocking others
    
-   Explicit minimum-rate guarantees
    

  

**Secondary**

-   Service fairness (implicit)
    
-   No instantaneous dominance
    

  

**Does NOT do**

-   Outcome fairness (completion time)
    
-   Bounded-lag fairness
    

  

ðŸ“Œ **True service-rate virtualization** in storage systems.

----------

### **8. Carbyne**

  

**Primary category**

-   âœ… **Outcome / finish-time fairness**
    

  

**Why**

-   Explicitly deprioritizes instantaneous fairness
    
-   Goal: no job finishes later than under strict fairness
    
-   Willing to violate resource fairness temporarily
    

  

**Secondary**

-   Service fairness as a _baseline constraint_
    

  

**Does NOT do**

-   Bounded-lag fairness
    
-   Rate virtualization as a contract
    

  

ðŸ“Œ **Counterfactual outcome fairness**.

----------

### **9. Themis**

  

**Primary category**

-   âœ… **Outcome / finish-time fairness**
    

  

**Why**

-   Defines fairness in terms of slowdown vs ideal
    
-   Explicitly optimizes max slowdown
    
-   Completion time is the fairness object
    

  

**Secondary**

-   Service fairness (as a constraint)
    

  

**Does NOT do**

-   Bounded-lag fairness
    
-   Rate virtualization
    

  

ðŸ“Œ **Most explicit finish-time fairness system**.

----------

### **10. HPC Moldable / Malleable Job Scheduling (theory)**

  

**Primary category**

-   âœ… **Rate-based / virtualization fairness** _(implicit)_
    

  

**Why**

-   Jobs scale down to fewer processors and run longer
    
-   Logical progress rate depends on assigned fraction
    

  

**Secondary**

-   Makespan / efficiency objectives (not fairness-driven)
    

  

**Does NOT do**

-   Explicit service fairness accounting
    
-   Outcome fairness across jobs (unless added)
    

  

ðŸ“Œ **Structural match**, but fairness is not the goal.

----------

## **Consolidated Table**

**Work**

**Instantaneous**

**Service-time**

**Bounded-lag**

**Rate-based**

**Outcome**

Linux CFS

â—

âœ…

âœ…

â—

âŒ

Hadoop Fair

â—

âœ…

âŒ

âŒ

âŒ

Delay Scheduling

â—

âœ…

â—

âŒ

âŒ

DRF (1D)

âŒ

âœ…

âŒ

âŒ

âŒ

Mesos

â—

âœ…

âŒ

âŒ

âŒ

YARN Capacity / H-DRF

â—

âœ…

âŒ

âŒ

âŒ

Pisces

â—

â—

âŒ

âœ…

âŒ

Carbyne

âŒ

â—

âŒ

âŒ

âœ…

Themis

âŒ

â—

âŒ

âŒ

âœ…

Moldable job theory

âŒ

âŒ

âŒ

âœ…

âŒ

Legend:

-   âœ… = primary notion
    
-   â— = secondary / supporting
    
-   âŒ = not a goal
    

----------

## **Key takeaway (important)**

  

There is a **clear split**:

-   **Schedulers (Hadoop, DRF, Mesos, YARN)**
    
    â†’ _Service fairness_ (what you consumed)
    
-   **OS schedulers (CFS)**
    
    â†’ _Bounded-lag service fairness_
    
-   **Storage / virtualization systems (Pisces)**
    
    â†’ _Rate-based fairness_
    
-   **Modern ML / cluster systems (Carbyne, Themis)**
    
    â†’ _Outcome fairness_
    

  

Your proposed model squarely sits in:

  

> **Rate-based / virtualization fairness**,

> which is _underrepresented_ in cluster schedulers but _very natural_ given malleability and non-preemption.

  

If you want next, we can:

-   formalize impossibility results between these categories,
    
-   show which notions dominate others,
    
-   or help you coin a **precise name + contract** for your fairness model (useful if this is for a paper).


> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTAwMDAyNzQxOCwzNjUxNDcxNzBdfQ==
-->